# Introducing LisaNetA simple implementation of a deep neural network to answer the most important question of all ...<p align="center">  <img src="https://github.com/tjards/deep-neural-network-play/blob/master/images/russian_cat.jpg" width="65%" /></p><div align="center"> **Is this a cat?**<div align="left"> The cat's name is Lisa. She is adorbable and belongs to my girlfriend's mother. We implement and compare both logistic regression and least squares regression for the cost function.The network has the following architecture:- 1 input layer (with 12288 nodes)- 3 hidden layers (with 20, 7, and 5 nodes; respectively)- 1 output layer (with 1 node)The input layers has 12288 nodes because the images are 64 pixels x 64 pixels and contain 3 RGB color layers (i.e. 64 x 64 x 3 = 12288)## Results The network correctly predicts the above photo as a 'cat' picture. Example output:<div align="center"> `y = 1.0, your model predicts a "cat" picture"`<div align="left"> The network uses batch learning. Here's an illustration of the performance over repeated iterations:<p align="center">  <img src="https://github.com/tjards/deep-neural-network-play/blob/master/results/progress.png" width="45%" />  <img src="https://github.com/tjards/deep-neural-network-play/blob/master/results/progress_mse.png" width="45%" /></p><div align="center"> **Reduction in cost** (a) when using logistic regression; (b) when using least squares regression<div align="left"> Since this is a classification problem (just 1's and 0's), logistic regression works better (note the faster reduction in cost above). The network makes some mistakes. Here is a print out of the incorrectly classified images when using logistic regression:<p align="center">  <img src="https://github.com/tjards/deep-neural-network-play/blob/master/results/mistakes.png" width="90%" /></p><div align="center"> <div align="left"> *Note: this code was developed in partial fufillment of the Deep Learning Specialization program under Andrew Ng et al.*